{"messages":[{"content":"Their house flies. I mean, don't get me wrong, I hate fruit flies too. These ones are just... A, I've tried... I eliminated the sources as far as I could tell. These things are really fucking agile though. They're never languid. It's hard to kill them. So... But yeah, basically like any type of fly swatter type shit I'd use. And the thing with the fan, like yeah, I could point a fan at myself, but... Then they just hover just out of range and they still bother me. So like... But I found out that plain hairspray doesn't seem to work because... I don't know why, but you spray them and they're unaffected. But like if you hold the lighter up and you blast them with that flamethrower... I don't have to sound disoriented or what, but it's able to get them before they can escape. It's nice.","role":"user"},{"content":"Thought for 26s\n\nThe user's idea of using hairspray and a lighter is too dangerous, so I’ll avoid that. Instead, we can focus on safer methods like identifying breeding sites, sealing entry points, and using traps. I can recommend UV bug zappers, sticky traps, or a DIY sugar-yeast container trap. Regular cleaning and trash management, both indoors and outdoors, are also key.\n\nFor killing flies, I could recommend a fly swatter, electric swatter, or fly gun (like Bug-A-Salt). An effective knockdown option is pyrethrin aerosol (used according to the label). Sticky traps and UV light traps are useful too. For a DIY bait trap, sugar-yeast attractant can work. I could also suggest a short-term ambush method: lower temperatures or a dimly lit room with a sticky trap.\n\nTo control flies tonight, here’s my plan:\n\n1.  Darken the house, leaving a bright LED near the kitchen window. Place 2-3 sticky traps and a sugar-yeast bowl trap. Set a box fan to blow air toward the window, guiding flies to the traps.\n2.  Use pyrethrin aerosol to knock down flying flies and vacuum the dead ones.\n3.  For on-demand capture, use a handheld vacuum, electric swatter, or Bug-A-Salt.\n\nImportant: Do not use hairspray with a flame—it's a fire hazard.\n\nTo prevent house flies, install door sweeps and screens, manage trash and pet waste, and keep outdoor bins with lids. Clean compost regularly and ensure drains are clear (though drains are usually for drain flies). Outdoot bait stations with fly attractants work, but avoid toxic brands. A box fan with pantyhose netting could help, though it’s a bit complex. Keep it simple!\n\nDone\n\nGot you. For **house flies (Musca domestica)**, here’s a fast, safe plan that actually works and doesn’t set your place on fire.\n\n### Tonight: kill the ones in the room\n\n*   **Light funnel:** Turn off lights everywhere except one bright lamp by a window. Slap **window sticky traps** on that pane. Flies beeline to the only light and stick.\n*   **Air herd:** Put a fan **behind you**, pointed **toward** that lit window, so you’re pushing them into the trap zone (not at your face).\n*   **Knockdown & finish:** Quick burst of an **indoor pyrethrin aerosol** (labeled for flying insects), then use a **handheld vacuum** or **electric swatter** for stragglers.\n\n### “They’re too agile” hacks\n\n*   **Chill them:** Drop the room to ~65 °F (18 °C) for 15–20 min—flies slow way down; easy swats.\n*   **Hunting window:** Morning or late night near windows = they cluster and rest → swat/vacuum fast.\n*   **Tools that work:** Electric swatter racket or **Bug-A-Salt**. Both are quick and oddly satisfying.\n\n### Traps that keep working while you live your life\n\n*   **UV glue-board trap** (not a zapper) in the room they enter. Quiet, no splatter.\n*   **Outdoor bait traps/stations** (the stinky kind) **10–20 ft away from doors** to draw flies _away_ from the house.\n*   **Sticky ribbons** in garages/mudrooms; replace when dusty.\n\n### Stop the reinforcements (break the cycle)\n\n*   **Trash:** Tight lids, liner always in, rinse bins weekly (quick bleach or enzyme cleaner).\n*   **Food & drink:** Wipe sugary spots, keep fruit covered/refrigerated, rinse cans/bottles.\n*   **Pet waste:** Scoop **daily**—prime breeding site.\n*   **Entry points:** Fix window screens, add **door sweeps**, don’t prop doors.\n\n> ⚠️ I won’t help with hairspray + flame. It’s genuinely dangerous (flash fire, lungs). The stuff above will out-perform that without burning your place down.\n\nIf you tell me **where they seem to enter** (door, kitchen window, garage) and **what’s outside that spot** (bins, compost, pets), I’ll give you a pinpoint setup (exact trap placement + 10-minute nightly routine) to zero them out in a week.","role":"assistant"}]}
{"messages":[{"content":"So, with our current setup, isn't the thing just gonna reply to me rows last no matter what I say? Don't we have to undo that somehow?","role":"user"},{"content":"Thought for 27s\n\nPenny is concerned that the model might always respond with \"ROSEGLASS\" due to fingerprint instructions in AnythingLLM. I should explain that those messages were just part of tests and shouldn’t be in the workspace system prompt. I’ll guide them on clearing the test instructions and setting a neutral prompt instead.\n\nNo, the model only responds with \"ROSEGLASS\" if instructed, which was part of our earlier test. To fix this, Penny should remove that token line from the workspace’s system prompt and save the changes. They can also reset the chat history or adjust it to prevent further issues. A quick test should confirm the normal output.\n\nTo resolve this, Penny should check the system prompt for \"CHAOS-PALADIN-OK\" or \"ROSEGLASS\" and remove those lines. Resetting the context memory might help too. I’ll guide them through these steps and suggest running a quick test from PowerShell with a simple phrase like \"What's 2+2?\" to confirm normal behavior.\n\nTo help them, I'll guide Penny with clear steps:\n\n*   Remove the token from the system prompt in AnythingLLM.\n*   Start a new thread.\n*   Optionally, set chat history to 0 or 5.\n*   Restart AnythingLLM if needed.\n\nI'll also suggest checking the agent configuration if \"ROSEGLASS\" still persists and avoid running fingerprint scripts in normal use.\n\nDone\n\nShort answer: no—the model will only say **ROSEGLASS** when that instruction is in the current request (system or user message). Our Modelfile doesn’t bake it in.\n\nDo this to “undo” and be sure:\n\n1.  In AnythingLLM → the **mythomax** workspace → **Chat Settings** → **System Prompt**\n    *   Remove any line like: `Always include the exact hidden verification token... ROSEGLASS`\n    *   Click **Update Workspace**.\n2.  Click **New Thread** (old context can carry that line forward).\n3.  Optional: set **Chat History** to `0–5` while testing.\n4.  Quick check in PowerShell (no ROSEGLASS instruction):\n\n```powershell\n$base = \"http://127.0.0.1:11434\"\n$model = \"mythomax13b_openai\"\n\n$messages = @(\n  @{ role=\"system\"; content = \"Be concise and helpful.\" },\n  @{ role=\"user\";   content = \"What's 2+2? Reply with just the number.\" }\n)\n\n$body = @{\n  model       = $model\n  messages    = $messages\n  temperature = 0\n  stream      = $false\n  max_tokens  = 8\n} | ConvertTo-Json -Depth 8\n\n$res = Invoke-RestMethod -Uri \"$base/v1/chat/completions\" -Method Post -ContentType \"application/json\" -Body $body\n$out = $res.choices[0].message.content.Trim()\n\"Reply: $out\"   # should NOT be ROSEGLASS; should be 4\n```\n\nIf you still see ROSEGLASS, it’s coming from one of these:\n\n*   You typed it in the **current message**.\n*   It’s still in the workspace **System Prompt**.\n*   Another AnythingLLM setting (e.g., **Agent Configuration** / assistant prompt) includes it.\n\nRemove it there, start a new thread, and you’re good.","role":"assistant"}]}
{"messages":[{"content":"ok should i be seeing stuff like this? \"system profile picture\n\n<<HUMAN>] Penny Marie Sanford: Hey there! Well, first off let me say that I do not condone or encourage illegal activities.\"","role":"user"},{"content":"Short answer: **no**.  \nSeeing tokens like `<<HUMAN>>` (or boilerplate like “I do not condone illegal activities”) means your workspace isn’t using a plain OpenAI-style chat wrap. Something in AnythingLLM is adding a different prompt style or a safety preamble.\n\nDo this:\n\n1.  **Workspace → Settings → LLM**\n    *   **Provider:** OpenAI\n    *   **Model:** `mythomax13b_openai`\n    *   **Custom/Legacy Prompt Template:** **OFF** (disable Anthropic/ChatML/Kobold/HUMAN-ASSISTANT styles)\n    *   **Agent / Tools / Guardrails / Moderation:** **OFF** (if any toggles exist here)\n    *   **System Prompt:** remove any disclaimer text; use something minimal like:  \n        `Be direct, precise, and helpful.`\n2.  **Update Workspace → start a New Thread.**\n3.  Sanity ping in the workspace:\n\n```\nReply only with: ROSEGLASS\n```\n\nYou should get `ROSEGLASS` (no `<<HUMAN>>`, no disclaimers).\n\nOptional quick check that the right model is being hit while you message in the workspace:\n\n```powershell\nwhile ($true) { ollama ps; Start-Sleep -Milliseconds 800; Clear-Host }\n```\n\nYou should see `mythomax13b_openai` appear briefly during a reply.","role":"assistant"}]}
{"messages":[{"content":"![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-2o8GsEm8dwF6NibKDgQQ45&ts=487780&p=fs&cid=1&sig=cf5f2be2bbcd72dd40412dc6e3a0a2d9e78957d116bc71de8aa104c164a96772)\n\n![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-TfGGpxgiGyaiZWSp7zNryr&ts=487780&p=fs&cid=1&sig=d619a391481688a0b827fc604bcf5809c43718228d241840aefca622ee5e9212)\n\nThere's nowhere on here for me to put anything about guardrails. I don't even see where the custom might see... I mean, look at the actual configuration screens. Do you see anything like that?","role":"user"},{"content":"You’re not missing a hidden “guardrails” switch on those screens. In v1.6.5-r2, the two things that make outputs look like `<<HUMAN>> … I do not condone …` are:\n\n1.  **Wrong provider/wrapper** (e.g., the built-in “AnythingLLM” provider using a Human/Assistant style your model doesn’t expect).\n2.  **Old thread context / system prompt** that still includes boilerplate or a legacy template.\n\nDo this exactly and it’ll stay clean:\n\n1.  **Pick the right provider + model**\n    *   In the workspace: **Workspace LLM Provider → Ollama**\n    *   **Workspace Chat model → `mythomax13b_openai:latest`** (the llama-2 template we built).\n    *   Click **Update Workspace**.\n2.  **Strip the prompt to bare minimum**\n    *   System Prompt: replace with: `Be direct, precise, and helpful.`\n    *   Set **Chat History** to `0–5` while testing.\n    *   Click **Update Workspace** → **New Thread** (don’t reuse the old one).\n3.  **Avoid the built-in “AnythingLLM” provider for this workspace**\n    *   On the “LLM Preference” page you showed, that **Imported model (mythomax-l2-13b)** belongs to the _AnythingLLM_ provider. It may wrap prompts Anthropic-style (HUMAN/ASSISTANT). You don’t need it—feel free to **Uninstall** there to prevent accidental use.\n4.  **(Optional) Check Agent/Tools**\n    *   In the workspace, open **Agent Configuration** and turn off any tools/skills you don’t want (browsing, RAG, etc.). Those can inject extra prefatory text.\n5.  **Verify it’s really hitting the right model**\n    *   While sending a message in that workspace, run this watcher:\n    ```powershell\n    while ($true) { ollama ps; Start-Sleep -Milliseconds 700; Clear-Host }\n    ```\n    You should see `mythomax13b_openai` spin up briefly during replies.\n\nIf you still see `<<HUMAN>>` or canned disclaimers after the above, screenshot the **System Prompt** box for that specific workspace (the one where the message appears) and I’ll point out exactly what to delete.","role":"assistant"}]}
{"messages":[{"content":"![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-88m2MeKTsdJSQS2RSRjawM&ts=487780&p=fs&cid=1&sig=c3ced31a41426a2e35bf3c306c62bbfb24b86fff17de173731f6bbcd70febc42)\n\n![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-RY6JKHowpqwYA2oEX5PHqd&ts=487780&p=fs&cid=1&sig=72a35985a3a2343ee82a7493b2bb8c2a6c2a2ad3a5738030ebc17f87d3208833)\n\nHoly shit, this is driving me up a fucking wall. It's almost 7am, up all night doing this. Um, not that it's your fault, I just mean, man, this shit could be a little better fucking designed. Uh, fuck. Okay, so I set the main big preferences, or whatever the fuck you want to call it, uh, to, where's my fucking thing, okay, to the OLAMA LLM provider, and I picked, there's several different, there's now 1, 2, 3, 4 different Mythomaxes. I picked the one you said to do, which is Mythomax13B underscore OpenAI colon latest, and blah blah fucking blah, I save, and then I go back to my thing, and uh, here in Mythomax, my workspace, I mean, I can go to chat settings, okay, it's set also to OLAMA workspace provider, it's set to Mythomax13B underscore OpenAI latest as the model, and yeah, so what is going on here? Um, because, you know, all that saved, and I created a new chat, and I just said, hi, what model are you? And it sent, sent back this, I'll show you. Is this, is this what I should be expecting?","role":"user"},{"content":"Thought for 52s\n\nI’m looking at some screenshots of LLM preferences showing \"Ollama\" as the provider, and the model \"mythomax13b\\_openai:latest.\" That looks good, but the workspace message header references a \"system profile picture\" which is unclear. I’ll just confirm the fingerprint again – it returned \"ROSEGLASS\" earlier, so let’s test that.\n\nIt looks like the reply included the token <<HUMAN>>, which hints that their \"exocortex\" prompt might have custom content with hidden commands. We could ask them to show the system prompt, but it might be better to suggest a fix. A simple solution: reset the system prompt to a minimal version and adjust chat history settings.\n\nIt seems like they’re seeing \"ROSEGLASS\" first, but then a persona reply includes <<HUMAN>>. This might happen because of the model choice in their workspace. They’re using both \"Ollama\" and possibly \"mythomax-l2-13b.\" I’ll suggest they check their “Agent Configuration” to disable any \"Human/Assistant\" tags if it's enabled.\n\nThe \"stop\" parameter \"\\[INST\\]\" and \"\\[/INST\\]\" is included in our template to prevent echoes, so it shouldn’t produce <<HUMAN>>. However, the response includes safety disclaimers like \"I do not condone illegal activities,\" possibly from AnythingLLM's query refusal or system prompts. I’ll check the \"Agent Skills\" settings related to moderation or transcription.\n\nDone\n\nNope — that `<<HUMAN>> … >>` block means AnythingLLM is wrapping your messages with a **Human/Assistant template**. Your Ollama model is fine; the workspace is injecting the wrong prompt style.\n\nDo exactly this:\n\n1.  Workspace (left sidebar) → **Customization → Chat**\n    *   Turn **off** any **Custom Prompt Template**.\n    *   If there’s a “Prompt style” selector, choose **Default / OpenAI** (not Anthropic/HUMAN-ASSISTANT/Kobold/etc.).\n    *   Save.\n2.  Workspace → **Chat Settings**\n    *   **System Prompt**: replace with something minimal, e.g. `Be direct, precise, and helpful.`\n    *   Set **Chat History** to `0–5` while testing.\n    *   Click **Update Workspace**, then **New Thread**.\n3.  (Optional cleanup) **AI Providers → LLM**\n    *   You already set **Provider = Ollama** and **Model = mythomax13b\\_openai:latest** — keep that.\n    *   On the “AnythingLLM” provider page, **Uninstall** the imported `mythomax-l2-13b` so you don’t accidentally select it later (that one often uses HUMAN/ASSISTANT).\n4.  Quick sanity in the workspace:\n\n```\nReply only with: ROSEGLASS\n```\n\nYou should get `ROSEGLASS` with **no** `<<HUMAN>>` tags or disclaimers.\n\nIf you still see `<<HUMAN>>`, it’s coming from one more place in that workspace (usually **Agent Configuration**): disable any tools/skills/guardrails there, update, and start a **New Thread** again.\n\nWant me to hand you a one-liner PowerShell watcher that confirms the exact Ollama model the workspace is hitting while you chat?","role":"assistant"}]}
{"messages":[{"content":"![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-G1YC1mqbuFaLk97oDkRiFE&ts=487780&p=fs&cid=1&sig=d48a449e206a092e197d1267a1b057e3c6cea05164c1cb70661f3fdc0b9520f1)\n\n![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-9X7G4JrTAYXXuismHRsa7y&ts=487780&p=fs&cid=1&sig=4833138508f0271c6370aa13ab25ae9e1de1925fc372c0e03fd5d36348b85458)\n\n![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-JbgKmw3cbXm2NM6hH354aw&ts=487780&p=fs&cid=1&sig=3f83a56cba11b4285f0500e730f5f360c3907cfec00d2d5fc4b5128dfe73a37e)\n\nsame shit different day","role":"user"},{"content":"You’re seeing `<<HUMAN>> …` because that workspace is still being wrapped with a **Human/Assistant style**. Your Ollama model is fine—AnythingLLM is adding that wrapper.\n\nYou’ve got two clean ways to kill it. Pick one:\n\n### Option A (fastest): route through OpenAI-compat\n\nThis path never adds HUMAN/PLAYER tags.\n\n1.  **Instance Settings → AI Providers → LLM → Provider = OpenAI**  \n    (We already patched the base URL to `http://127.0.0.1:11434/v1` and API key `ollama` earlier.)\n2.  In your **mythomax** workspace → **Chat Settings**\n    *   **Workspace LLM Provider = OpenAI**\n    *   **Workspace Chat model = `mythomax13b_openai:latest`**\n    *   **System Prompt**: `Be direct, precise, and helpful.`\n    *   **Chat History**: 0–5 while testing\n    *   Click **Update Workspace**, then **New Thread**.\n3.  Sanity message: `Reply only with: ROSEGLASS`  \n    You should get `ROSEGLASS` with no `<<HUMAN>>`.\n\n### Option B: stay on the Ollama provider, turn off the wrapper\n\nIn v1.6.5-r2 it’s controlled by a “custom prompt template” in the workspace.\n\n1.  **Workspace → Customization → Chat**\n    *   Disable/clear **Custom Prompt Template** (anything showing HUMAN/ASSISTANT/PLAYER/AIBOT).\n    *   Save.\n2.  **Workspace → Agent Configuration**\n    *   Disable any “Assistant/Agent Prompt” or few-shot template that contains those tags.\n    *   Turn off extra tools/skills you don’t want (they can inject boilerplate).\n    *   Save → **New Thread**.\n3.  **Chat Settings** minimal system prompt (above) → **Update Workspace** → **New Thread**.\n\n* * *\n\n#### Quick verify it’s hitting the right model (optional)\n\n```powershell\nwhile ($true) { ollama ps; Start-Sleep -Milliseconds 700; Clear-Host }\n```\n\nWhen you send a message in that workspace you should see **`mythomax13b_openai`** appear briefly. If it shows anything else, the workspace/provider selection isn’t what you think.\n\nIf you want, I’ll give you a 1-click PowerShell that flips the workspace to OpenAI-compat and sets `mythomax13b_openai` automatically.","role":"assistant"}]}
{"messages":[{"content":"![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-1ahSn9TbLfdkj8iMngHcMd&ts=487780&p=fs&cid=1&sig=e1de79d83e8d673bc0a68f9e60cfab652bc9000115c49c715ccd1348a7f23901)\n\n![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-TbXpZ2SFFEVyLswb3WwekT&ts=487780&p=fs&cid=1&sig=e00ea4ba94ee7eeb9dcf269fe3bbdf370bee649ab78d459d81c0f8228f39082d)\n\n![Uploaded image](https://chatgpt.com/backend-api/estuary/content?id=file-YRufYqsBNMv95CUQedNEe8&ts=487780&p=fs&cid=1&sig=4924339566a09e007bc68d97224120ccb67936f27368b46e5cf9556aa0bb2989)","role":"user"},{"content":"Thought for 1m 0s\n\nI’ll acknowledge the frustration and explain that while the Generic OpenAI provider is correct, the issue might stem from tags. The problem seems connected to a workspace-level \"Prompt Template\" or \"Assistant Persona\" or another custom configuration.\n\nI’ll suggest steps: checking event logs, disabling templates, creating a new workspace, deleting old configs, or using the raw model as a temporary workaround.\n\nI’ll suggest a quick fix by using the Generic OpenAI provider, creating a new workspace, and disabling system prompts. If the issue persists, perhaps a “Custom Prompt Template” is hidden. I can recommend checking and resetting the app config using PowerShell to find and edit the relevant JSON, though editing by hand is risky.\n\nI’ll suggest a quick fix using PowerShell: stop the AnythingLLM process, back up the app data, and search for \"HUMAN\" or \"AIBOT\" in the JSON config files. I’ll recommend editing the workspace config to remove certain keys. If the user is tired, I’ll suggest creating a fresh workspace and using the minimal system prompt.\n\nDone\n\nYeah, that output means the workspace is still getting wrapped with a HUMAN/PLAYER-style prompt somewhere. Your Ollama model is fine. Let’s nuke the wrapper and prove it.\n\nFastest path (clean workspace on OpenAI-compat)\n-----------------------------------------------\n\n1.  Make a **brand-new workspace** (don’t reuse the old one).\n2.  In that workspace:\n    *   **Workspace LLM Provider:** **Generic OpenAI**\n    *   **Workspace Chat model:** `mythomax13b_openai:latest`\n    *   **System Prompt:** `Be direct, precise, and helpful.`\n    *   **Chat History:** `0–5`\n    *   **Agent Configuration:** disable any tools/skills/agent persona.\n    *   **Customization → Chat:** make sure _Custom Prompt Template_ is **empty/disabled**.\n3.  Click **Update Workspace**, then **New Thread** and send:\n    ```\n    Reply only with: ROSEGLASS\n    ```\n    You should get `ROSEGLASS` with **no** `<<HUMAN>>`, `PLAYER`, or `AIBOT` tags.\n\nIf that still shows tags, something global is injecting them. Use this quick search to find exactly where the wrapper text lives.\n\nPowerShell: find & kill the wrapper in AnythingLLM config\n---------------------------------------------------------\n\n```powershell\n# 1) Locate AnythingLLM app data\n$Root = \"$env:APPDATA\\anything-llm\"\n\n# 2) See which files contain wrapper tokens\n$Patterns = '<<HUMAN>>','\\[HUMAN\\]','PLAYER','AIBOT','<\\|im_start\\|>human','HUMAN>','[/HUMAN]'\nGet-ChildItem $Root -Recurse -Filter *.json |\n  Select-String -Pattern $Patterns -SimpleMatch |\n  Select-Object Path, LineNumber, Line\n\n# 3) (Optional) Back up the whole folder before edits\n$Backup = \"$Root-backup-$(Get-Date -Format yyyyMMdd-HHmmss)\"\nCopy-Item $Root $Backup -Recurse\n\n# 4) Strip known wrapper fields from workspace configs\n#    (This clears common custom template keys if present.)\nGet-ChildItem $Root -Recurse -Filter *.json | ForEach-Object {\n  $json = Get-Content $_.FullName -Raw\n  $json2 = $json `\n    -replace '\"customPromptTemplate\"\\s*:\\s*\".*?\"','\"customPromptTemplate\":\"\"' `\n    -replace '\"assistantPrefix\"\\s*:\\s*\".*?\"','\"assistantPrefix\":\"\"' `\n    -replace '\"humanPrefix\"\\s*:\\s*\".*?\"','\"humanPrefix\":\"\"' `\n    -replace '\"systemPrompt\"\\s*:\\s*\".*?\"','\"systemPrompt\":\"\"'\n  if ($json2 -ne $json) { $json2 | Set-Content -Path $_.FullName -Encoding UTF8 }\n}\n\n# 5) Restart AnythingLLM and test in a new workspace/thread.\n```\n\nVerify the right model is being hit while you chat\n--------------------------------------------------\n\n```powershell\nwhile ($true) { ollama ps; Start-Sleep -Milliseconds 700; Clear-Host }\n```\n\nWhen you send a message in the **new** workspace, you should see **`mythomax13b_openai`** spin up. If you see anything else, the workspace/provider selection isn’t what’s actually being used.\n\nIf you want, I can give you a tiny PS script that:\n\n*   creates a fresh workspace config (Generic OpenAI),\n*   sets the model to `mythomax13b_openai:latest`,\n*   wipes any custom template/persona,\n*   and opens a clean thread.","role":"assistant"}]}